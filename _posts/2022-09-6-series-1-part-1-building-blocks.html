---
layout: post
title:  "Part 1 - Building Blocks"
date:   2022-09-12 08:27:05 +0300
categories: ["Data Architecture","Data Platform"]
description : Architecture Design
excerpt : In this part of the series, we explore the building blocks of the data platform architecture design
permalink: building-blocks-series-1-part-1.html
is_series: true
is_pinned: false
is_featured: false
series_title: "Deploying a Cost Optimized Resilient Data Platform"
---

<h3>{{ page.title | upcase }} </h3>
<br>

<p>
    By definition, a data platform is a complete solution for ingesting, processing , analyzing, and presenting the data generated by systems, processes, and infrastructure
</p>

<p>The following will be used as building blocks during the data platform architecture design:</p>
<ul class="list-unstyled">
    <li>• Infrastructure Design</li>
    <li>• Secrets Management</li>
    <li>• Observability</li>
    <li>• Lineage</li>
    <li>• Orchestration and Task Registration</li>
    <li>• Data Certification</li>
    <li>• Query Service</li>
    <li>• Data Subscription</li>
    <li>• Storage Service</li>
    <li>• Data Transformations</li>
    <li>• Data Subscription</li>
    <li>• Data Governance</li>
    <li>• Data Discovery</li>
    <li>• Security</li>
</ul>

<p>Below is a brief overview of these building blocks:</p>

<h3>Infrastructure</h3>
<p>
    The services require infrastructure on which to ran, cloud service offerings range from IaaS, PaaS to SaaS, our focus will be IaaS and PaaS
</p>
<p>We will work under the assumption that we work for an AWS shop, our infrastructure will be built on AWS, our data pipelines shouldn’t be tightly coupled with the cloud provider
    service offering, they should be easily portable to a different cloud provider
</p>
<p>
    We should be able to provision and allocate the right number of resources to execute tasks and continuously monitor the resource usage
</p>
<p>
    We should manage our infrastructure using Infrastructure as Code (IaC) and version control as it engenders Idempotency and keeps track of changes to the infrastructure, it also
    helps with prototyping as it enables you to spin up an environment at a moment’s notice, isolating development, and production environments
</p>
<h3>Observability</h3>
<p>
    We need to have an operational view of our pipelines and the data platform infrastructure resources, these can be delivered through dashboards, alerting -- for both expected
    events and any anomalies.
</p>
<p>
    We should also be able to track jobs within our pipeline for status and identify reasons for failure should they occur
</p>
<p>
    Pipeline metadata like data freshness, could also be published to these observability dashboards
</p>
<p>We will put in place a centralized logging mechanism for observability</p>
<h3>Secrets management</h3>
<p>
    We need to securely store our passwords and rotated without causing significant downtime
</p>
<h3>Orchestration and Task Registration</h3>
<p>
    Analytics workloads (Jobs ) are a bunch of long running tasks and therefore need to be orchestrated by a state machine, we need to identify the order of the execution, then
    have the orchestration service build a data pipeline graph.
</p>
<p>
    The orchestration service will initiate and track the execution of task; the orchestration service will serve as our task registry
</p>
<h3>Data Lineage</h3>
<p>
    During transformations, tasks will have to provide some sort of indication of their inputs and outputs, this metadata will be used by the orchestration service to draw up the
    DAG graph.
</p>
<p>
    The lineage graph represents relationships between a collection of tasks, this helps with traceability of dependencies
</p>
<h3>Data Certification</h3>
<p>
    We shall have to embed data quality checks in between tasks to ensure only good quality data gets to dashboards and the subscription service
</p>
<h3>Data Discovery</h3>
<p>
    To create data democratization and eliminate tribal knowledge within our organization, we will need a single searchable portal from where data users will get answers to
    questions like:
</p>
<ul class="list-unstyled">
    <li>• Where can I find data about a verb X, noun Y ?</li>
    <li>• Who owns certain data sets and how the data is created, Is it trustable?</li>
    <li>• Column level statistics, e.g., Mean, Standard deviation</li>
    <li>• The frequently the data is refreshed</li>
    <li>• Orchestration and Task Registration</li>
    <li>• What is the context of the data</li>
</ul>
<h3>Data Subscription Service</h3>
<p>
    We shall have to make it possible to integrate with external services, these could be dashboards, reporting engines, and Rest APIs. These consumers should be able to subscribe
    to this data subscription service
</p>

<h3>Query Service</h3>
<p>
    Upon ingestion of data into the storage layer, it will be accessible through a query service, transformation tasks will use this service to fetch and persist data into the
    storage layers
</p>
<p>
    This is a layer over an SQL query engine or data warehouse, it should be able to scale depending on the complexity and concurrency of the queries
</p>
<h3>Data Governance</h3>
<p>
    We should be able to define authentication and fine-grained access controls on the data resident in our data stores. This can be implemented within the query service; these
    access controls should enforce authorization mechanism to limit on what users can have access to
</p>
<h3>Data Transformation</h3>
<p>
    When creating transformation tasks, we should be able to generate metadata which includes upstream dependencies, these dependencies are used by our orchestration tool to
    dynamically generate our transformation DAGs and lineage
</p>
<p>Transformations can either be Stream processing or batch processing depending on the need, our architecture covers both, this kind of architecture is called a Lambda
    Architecture</p>
<h3>Security</h3>
<p>
    Security considerations will be taken on data in transit and at rest
</p>
